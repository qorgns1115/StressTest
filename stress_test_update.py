import subprocess
import json
import time
import pandas as pd
import os
from datetime import datetime
import requests
from typing import Dict, Tuple, Optional
import numpy as np
from json import JSONEncoder

class NumpyEncoder(JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)


# API ì—”ë“œí¬ì¸íŠ¸ ë° HTTP ë©”ì„œë“œ ì„¤ì •
API_ENDPOINTS = {
    "/v1/user/verify": "POST",
    "/v1/system/health": "GET",
    "/v1/comms/member": "POST",
    "/v1/comms/mobile-join": "POST",
    "/v1/comms/mobile-usage": "POST",
    "/v1/comms/mobile-bills": "POST",
    "/v1/comms/mobile-payments": "POST"
}

# ì—”ë“œí¬ì¸íŠ¸ë³„ Header ì„¤ì •
ENDPOINT_HEADERS = {
    "/v1/user/verify": {
        "X-Api-Tx-Id": "12345",
        "X-Src-Inst-Cd": "SRC001",
        "X-Dst-Inst-Cd": "DST001",
        "Content-Type": "application/json"
    },
    "/v1/system/health": {
        "X-Api-Tx-Id": "12345"
    },
    "default": {
        "X-Api-Tx-Id": "12345",
        "X-Src-Inst-Cd": "SRC001",
        "X-Dst-Inst-Cd": "DST001",
        "X-Api-Type": "application/json",
        "Content-Type": "application/json"
    }
}

# Request Bodies ì„¤ì •
REQUEST_BODIES = {
    "/v1/user/verify": '{"user_id": "test_user"}',
    "/v1/comms/member": '{"user_id": "test_user", "search_timestamp": "2025-02-10", "next_page": "2000","limit": 1}',
    "/v1/comms/mobile-join": '{"user_id": "test_id", "search_timestamp": "2025-02-10", "ctrt_mng_no": "100001"}',
    "/v1/comms/mobile-usage": '{"user_id": "test_user", "search_timestamp": "2025-02-10", "ctrt_mng_no": "20", "bgng_ym": "2024-12", "end_ym": "2025-01"}',
    "/v1/comms/mobile-bills": '{"user_id": "test_user", "search_timestamp": "2025-02-10", "ctrt_mng_no": "20", "bgng_ym": "2024-12", "end_ym": "2025-01"}',
    "/v1/comms/mobile-payments": '{"user_id": "test_user", "search_timestamp": "2025-02-10", "ctrt_mng_no": "20", "bgng_ym": "2024-12", "end_ym": "2025-01"}'
}

# JMeter ì‹¤í–‰ ê²½ë¡œ
JMETER_PATH = r"C:\Users\Administrator\Downloads\apache-jmeter-5.6.3\apache-jmeter-5.6.3\bin\jmeter.bat"

def run_jmeter_test(jmx_file, results_dir):
    """
    JMeter í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    :param jmx_file: JMeter í…ŒìŠ¤íŠ¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
    :param results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    :return: (ì„±ê³µ ì—¬ë¶€, ê²°ê³¼ íŒŒì¼ ê²½ë¡œ)
    """
    result_file = os.path.join(results_dir, "test_results.jtl")
    log_file = os.path.join(results_dir, "jmeter.log")
    
    print(f"ğŸš€ JMeter í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    print(f"ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: {results_dir}")
    
    cmd = f'"{JMETER_PATH}" -n -t "{jmx_file}" -l "{result_file}" -j "{log_file}"'
    
    try:
        process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                print(output.strip())
        
        return_code = process.poll()
        
        if return_code == 0:
            print("âœ… JMeter í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            return True, result_file
        else:
            print(f"âŒ JMeter ì‹¤í–‰ ì‹¤íŒ¨ (ë°˜í™˜ ì½”ë“œ: {return_code})")
            return False, None
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False, None
    
def load_config(config_file='stresstest_config.json'):
    """ì„¤ì • íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ êµ¬ì„± ë¡œë“œ"""
    try:
        with open(config_file, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_file}")
        exit(1)
    except json.JSONDecodeError:
        print(f"âŒ ì„¤ì • íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {config_file}")
        exit(1)

class StressTestController:
    def __init__(self, config):
        """
        ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì»¨íŠ¸ë¡¤ëŸ¬ ì´ˆê¸°í™”
        """
        self.initial_threads = config['test_parameters']['initial_threads']
        self.thread_increment = config['test_parameters']['thread_increment']
        self.max_threads = config['test_parameters']['max_threads']
        self.error_threshold = config['test_parameters']['error_threshold']
        self.response_time_threshold = config['test_parameters']['response_time_threshold']
        self.initial_duration = config['test_parameters']['initial_duration']
        self.duration_increment = config['test_parameters']['duration_increment']
        self.max_duration = config['test_parameters']['max_duration']
        
        self.current_threads = self.initial_threads
        self.current_duration = self.initial_duration
        self.test_phase = "running"
        self.failure_detected = False
        self.failure_reason = None
        
    def should_continue(self, stats: Dict) -> Tuple[bool, Optional[str]]:
        """
        í…ŒìŠ¤íŠ¸ ì§€ì† ì—¬ë¶€ ê²°ì •
        """
        # max threadì—ì„œ ì„ê³„ê°’ ì´ˆê³¼í•˜ë©´ í…ŒìŠ¤íŠ¸ ì¢…ë£Œ
        if self.current_threads >= self.max_threads:
            if stats["error_rate"] > self.error_threshold:
                return False, f"ìµœëŒ€ ì“°ë ˆë“œ ìˆ˜({self.max_threads})ì—ì„œ ì˜¤ë¥˜ìœ¨({stats['error_rate']}%)ì´ ì„ê³„ê°’({self.error_threshold}%)ì„ ì´ˆê³¼í•¨"
            if stats["avg_response_time"] > self.response_time_threshold:
                return False, f"ìµœëŒ€ ì“°ë ˆë“œ ìˆ˜({self.max_threads})ì—ì„œ ì‘ë‹µì‹œê°„({stats['avg_response_time']}ms)ì´ ì„ê³„ê°’({self.response_time_threshold}ms)ì„ ì´ˆê³¼í•¨"
            
        # max thread & max duration ë„ë‹¬ ì‹œ ì¢…ë£Œ
        if self.current_threads >= self.max_threads and self.current_duration >= self.max_duration:
            return False, f"ìµœëŒ€ ì“°ë ˆë“œ ìˆ˜({self.max_threads})ì™€ ìµœëŒ€ ì§€ì†ì‹œê°„({self.max_duration}ì´ˆ)ì— ë„ë‹¬í•¨"
            
        # ê·¸ ì™¸ì˜ ê²½ìš° ì„ê³„ê°’ ì´ˆê³¼ ì‹œ thread ì¦ê°€ë¥¼ ìœ„í•œ ì‹œê·¸ë„ ë°˜í™˜
        if stats["error_rate"] > self.error_threshold:
            print(f"âš ï¸ ì˜¤ë¥˜ìœ¨({stats['error_rate']}%)ì´ ì„ê³„ê°’({self.error_threshold}%)ì„ ì´ˆê³¼í•¨")
            return True, "threshold_exceeded"
        
        if stats["avg_response_time"] > self.response_time_threshold:
            print(f"âš ï¸ ì‘ë‹µì‹œê°„({stats['avg_response_time']}ms)ì´ ì„ê³„ê°’({self.response_time_threshold}ms)ì„ ì´ˆê³¼í•¨")
            return True, "threshold_exceeded"
            
        return True, None

    def increment_test_parameters(self, threshold_exceeded=False):
        """ë‹¤ìŒ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ì¡°ì •"""
        if threshold_exceeded:
            # ì„ê³„ê°’ ì´ˆê³¼ ì‹œ duration ì´ˆê¸°í™”í•˜ê³  thread ì¦ê°€
            self.current_duration = self.initial_duration
            if self.current_threads < self.max_threads:
                self.current_threads += self.thread_increment
            return "threads_increased"
        
        # ì •ìƒì ì¸ ê²½ìš° duration ì¦ê°€
        if self.current_duration < self.max_duration:
            self.current_duration += self.duration_increment
        else:
            self.current_duration = self.initial_duration
            if self.current_threads < self.max_threads:
                self.current_threads += self.thread_increment
            return "threads_increased"


def create_jmx_file(config, results_dir, thread_count, duration, filename="generated_test.jmx"):
    """JMeter í…ŒìŠ¤íŠ¸ ì„¤ì • íŒŒì¼ ìƒì„±"""
    full_path = os.path.join(results_dir, filename)
    
    # (ì´ì „ JMX í…œí”Œë¦¿ ì½”ë“œëŠ” ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ë˜, duration ê°’ë§Œ ë³€ê²½)
    # ThreadGroup ì„¤ì •ì—ì„œ duration ê°’ì„ ë³€ê²½:
    # <stringProp name="ThreadGroup.duration">{duration}</stringProp>
    jmx_template = f'''<?xml version="1.0" encoding="UTF-8"?>
<jmeterTestPlan version="1.2" properties="5.0" jmeter="5.6.3">
  <hashTree>
    <TestPlan guiclass="TestPlanGui" testclass="TestPlan" testname="Stress Test Plan" enabled="true">
      <stringProp name="TestPlan.comments"></stringProp>
      <boolProp name="TestPlan.functional_mode">false</boolProp>
      <boolProp name="TestPlan.tearDown_on_shutdown">true</boolProp>
      <boolProp name="TestPlan.serialize_threadgroups">false</boolProp>
      <elementProp name="TestPlan.user_defined_variables" elementType="Arguments" guiclass="ArgumentsPanel" testclass="Arguments" testname="User Defined Variables" enabled="true">
        <collectionProp name="Arguments.arguments"/>
      </elementProp>
      <stringProp name="TestPlan.user_define_classpath"></stringProp>
    </TestPlan>
    <hashTree>'''

    # ê° API ì—”ë“œí¬ì¸íŠ¸ì— ëŒ€í•œ ì“°ë ˆë“œ ê·¸ë£¹ ì„¤ì •
    for endpoint, method in API_ENDPOINTS.items():
        headers = ENDPOINT_HEADERS.get(endpoint, ENDPOINT_HEADERS['default'])
        body = REQUEST_BODIES.get(endpoint, "")
        
        jmx_template += f''' #ê° ì—”ë“œí¬ì¸íŠ¸ì— ëŒ€í•œ ì“°ë ˆë“œ ê·¸ë£¹ ì–´ë–»ê²Œ ì„¤ì •í•  ì§€?
      <ThreadGroup guiclass="ThreadGroupGui" testclass="ThreadGroup" testname="{endpoint} Test" enabled="true">
        <stringProp name="ThreadGroup.on_sample_error">continue</stringProp>
        <elementProp name="ThreadGroup.main_controller" elementType="LoopController" guiclass="LoopControlPanel" testclass="LoopController" testname="Loop Controller" enabled="true">
          <boolProp name="LoopController.continue_forever">false</boolProp>
          <stringProp name="LoopController.loops">1</stringProp>
        </elementProp>
        <stringProp name="ThreadGroup.num_threads">{thread_count}</stringProp>
        <stringProp name="ThreadGroup.ramp_time">1</stringProp>
        <boolProp name="ThreadGroup.scheduler">true</boolProp>
        <stringProp name="ThreadGroup.duration">{duration}</stringProp>
        <stringProp name="ThreadGroup.delay"></stringProp>
        <boolProp name="ThreadGroup.same_user_on_next_iteration">true</boolProp>
      </ThreadGroup>
      <hashTree>
        <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="{endpoint}" enabled="true">
          <boolProp name="HTTPSampler.postBodyRaw">true</boolProp>
          <elementProp name="HTTPsampler.Arguments" elementType="Arguments">
            <collectionProp name="Arguments.arguments">'''

        if method == "POST" and body:
            jmx_template += f'''
              <elementProp name="" elementType="HTTPArgument">
                <boolProp name="HTTPArgument.always_encode">false</boolProp>
                <stringProp name="Argument.value">{body}</stringProp>
                <stringProp name="Argument.metadata">=</stringProp>
              </elementProp>'''

        jmx_template += f'''
            </collectionProp>
          </elementProp>
          <stringProp name="HTTPSampler.domain">{config['server_name']}</stringProp>
          <stringProp name="HTTPSampler.port">{config['port']}</stringProp>
          <stringProp name="HTTPSampler.protocol">{config['protocol']}</stringProp>
          <stringProp name="HTTPSampler.contentEncoding">UTF-8</stringProp>
          <stringProp name="HTTPSampler.path">{endpoint}</stringProp>
          <stringProp name="HTTPSampler.method">{method}</stringProp>
          <boolProp name="HTTPSampler.follow_redirects">true</boolProp>
          <boolProp name="HTTPSampler.auto_redirects">false</boolProp>
          <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
          <boolProp name="HTTPSampler.DO_MULTIPART_POST">false</boolProp>
          <stringProp name="HTTPSampler.embedded_url_re"></stringProp>
          <stringProp name="HTTPSampler.connect_timeout"></stringProp>
          <stringProp name="HTTPSampler.response_timeout"></stringProp>
        </HTTPSamplerProxy>
        <hashTree>
          <HeaderManager guiclass="HeaderPanel" testclass="HeaderManager" testname="HTTP Header Manager" enabled="true">
            <collectionProp name="HeaderManager.headers">'''

        for header_name, header_value in headers.items():
            jmx_template += f'''
              <elementProp name="" elementType="Header">
                <stringProp name="Header.name">{header_name}</stringProp>
                <stringProp name="Header.value">{header_value}</stringProp>
              </elementProp>'''

        jmx_template += '''
            </collectionProp>
          </HeaderManager>
          <hashTree/>
        </hashTree>
      </hashTree>'''

    # ê²°ê³¼ ìˆ˜ì§‘ê¸° ì¶”ê°€
    jmx_template += '''
      <ResultCollector guiclass="ViewResultsFullVisualizer" testclass="ResultCollector" testname="View Results Tree" enabled="true">
        <boolProp name="ResultCollector.error_logging">false</boolProp>
        <objProp>
          <name>saveConfig</name>
          <value class="SampleSaveConfiguration">
            <time>true</time>
            <latency>true</latency>
            <timestamp>true</timestamp>
            <success>true</success>
            <label>true</label>
            <code>true</code>
            <message>true</message>
            <threadName>true</threadName>
            <dataType>true</dataType>
            <encoding>false</encoding>
            <assertions>true</assertions>
            <subresults>true</subresults>
            <responseData>false</responseData>
            <samplerData>false</samplerData>
            <xml>false</xml>
            <fieldNames>true</fieldNames>
            <responseHeaders>false</responseHeaders>
            <requestHeaders>false</requestHeaders>
            <responseDataOnError>false</responseDataOnError>
            <saveAssertionResultsFailureMessage>true</saveAssertionResultsFailureMessage>
            <assertionsResultsToSave>0</assertionsResultsToSave>
            <bytes>true</bytes>
            <sentBytes>true</sentBytes>
            <url>true</url>
            <threadCounts>true</threadCounts>
            <idleTime>true</idleTime>
            <connectTime>true</connectTime>
          </value>
        </objProp>
        <stringProp name="filename"></stringProp>
      </ResultCollector>
      <hashTree/>
    </hashTree>
  </hashTree>
</jmeterTestPlan>'''

    with open(full_path, 'w', encoding='utf-8') as f:
        f.write(jmx_template)
    
    with open(full_path, 'w', encoding='utf-8') as f:
        f.write(jmx_template)

    return full_path
    

def analyze_results(jtl_file: str, results_dir: str) -> Dict:
    """
    í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
    :param jtl_file: JMeter ê²°ê³¼ íŒŒì¼ (.jtl)
    :param results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    :return: ë¶„ì„ëœ í†µê³„ ì •ë³´
    """
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì¤‘... ({jtl_file})")
    
    # JTL íŒŒì¼ ì½ê¸°
    df = pd.read_csv(jtl_file)
    
    # ê¸°ë³¸ í†µê³„ ê³„ì‚°
    stats = {
        "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "total_requests": len(df),
        "error_count": (df['success'] == False).sum(),
        "error_rate": float((df['success'] == False).mean() * 100),
        
        # ì‘ë‹µ ì‹œê°„ í†µê³„ (ë°€ë¦¬ì´ˆ)
        "response_time": {
            "min": float(df['elapsed'].min()),
            "max": float(df['elapsed'].max()),
            "mean": float(df['elapsed'].mean()),
            "median": float(df['elapsed'].median()),
            "90th_percentile": float(df['elapsed'].quantile(0.90)),
            "95th_percentile": float(df['elapsed'].quantile(0.95)),
            "99th_percentile": float(df['elapsed'].quantile(0.99))
        },
        
        # ì²˜ë¦¬ëŸ‰ í†µê³„
        "throughput": {
            "requests_per_second": float(len(df) / (df['timeStamp'].max() - df['timeStamp'].min()) * 1000),
            "total_bytes": int(df['bytes'].sum()),
            "avg_bytes_per_request": float(df['bytes'].mean())
        },
        
        # ì—ëŸ¬ ìƒì„¸ ì •ë³´
        "errors": df[df['success'] == False]['responseMessage'].value_counts().to_dict(),
        
        # HTTP ì‘ë‹µ ì½”ë“œ ë¶„í¬
        "response_codes": df['responseCode'].value_counts().to_dict()
    }
    
    # ì—”ë“œí¬ì¸íŠ¸ë³„ í†µê³„
    endpoint_stats = {}
    for endpoint in df['label'].unique():
        endpoint_df = df[df['label'] == endpoint]
        endpoint_stats[endpoint] = {
            "total_requests": len(endpoint_df),
            "error_rate": float((endpoint_df['success'] == False).mean() * 100),
            "avg_response_time": float(endpoint_df['elapsed'].mean()),
            "90th_percentile": float(endpoint_df['elapsed'].quantile(0.90)),
            "error_count": int((endpoint_df['success'] == False).sum())
        }
    
    stats["endpoint_statistics"] = endpoint_stats
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    json_results_file = os.path.join(results_dir, f"test_results_{timestamp}.json")
    
    # ì´ë ‡ê²Œ ì‚¬ìš©:
    with open(json_results_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=4, ensure_ascii=False, cls=NumpyEncoder)
    
    print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {json_results_file}")
    
    # ìš”ì•½ ë¡œê·¸ íŒŒì¼ ìƒì„±
    summary_file = os.path.join(results_dir, f"test_summary_{timestamp}.txt")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½\n")
        f.write(f"í…ŒìŠ¤íŠ¸ ì‹œê°„: {stats['timestamp']}\n")
        f.write(f"ì´ ìš”ì²­ ìˆ˜: {stats['total_requests']}\n")
        f.write(f"ì˜¤ë¥˜ìœ¨: {stats['error_rate']:.2f}%\n")
        f.write(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {stats['response_time']['mean']:.2f}ms\n")
        f.write(f"90th ë°±ë¶„ìœ„ ì‘ë‹µ ì‹œê°„: {stats['response_time']['90th_percentile']:.2f}ms\n")
        f.write(f"ì´ˆë‹¹ ìš”ì²­ ìˆ˜: {stats['throughput']['requests_per_second']:.2f}\n")
        
        f.write("\nì—”ë“œí¬ì¸íŠ¸ë³„ í†µê³„:\n")
        for endpoint, endpoint_stat in stats["endpoint_statistics"].items():
            f.write(f"\n{endpoint}:\n")
            f.write(f"  ì´ ìš”ì²­ ìˆ˜: {endpoint_stat['total_requests']}\n")
            f.write(f"  ì˜¤ë¥˜ìœ¨: {endpoint_stat['error_rate']:.2f}%\n")
            f.write(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {endpoint_stat['avg_response_time']:.2f}ms\n")
    
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_file}")
    
    return stats

def run_stress_test(config: Dict):
    """
    ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜
    """
    base_results_dir = f"stress_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.makedirs(base_results_dir, exist_ok=True)
    
    # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì»¨íŠ¸ë¡¤ëŸ¬ ì´ˆê¸°í™”
    controller = StressTestController(config)
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì • ê¸°ë¡
    with open(os.path.join(base_results_dir, "test_config.json"), 'w') as f:
        json.dump(config, f, indent=4)
    
    while True:
        phase_dir = os.path.join(base_results_dir, 
                               f"phase_threads_{controller.current_threads}_duration_{controller.current_duration}")
        os.makedirs(phase_dir, exist_ok=True)
        
        print(f"\nğŸ”„ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ ì‹œì‘:")
        print(f"   - ì“°ë ˆë“œ ìˆ˜: {controller.current_threads}")
        print(f"   - í…ŒìŠ¤íŠ¸ ì§€ì†ì‹œê°„: {controller.current_duration}ì´ˆ")
        
        # JMeter í…ŒìŠ¤íŠ¸ ìƒì„± ë° ì‹¤í–‰
        jmx_file = create_jmx_file(config['server_config'], phase_dir, 
                                  controller.current_threads, controller.current_duration)
        success, result_file = run_jmeter_test(jmx_file, phase_dir)
        
        if not success:
            print("âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨")
            controller.failure_detected = True
            controller.failure_reason = "JMeter ì‹¤í–‰ ì‹¤íŒ¨"
            break
            
        # ê²°ê³¼ ë¶„ì„
        stats = analyze_results(result_file, phase_dir)
        
        # ë‹¨ê³„ë³„ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ë‹¨ê³„ë³„ ê²°ê³¼:")
        print(f"ì“°ë ˆë“œ ìˆ˜: {controller.current_threads}")
        print(f"í…ŒìŠ¤íŠ¸ ì§€ì†ì‹œê°„: {controller.current_duration}ì´ˆ")
        print(f"ì´ˆë‹¹ ìš”ì²­ ìˆ˜: {stats['throughput']['requests_per_second']:.2f}")
        print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {stats['response_time']['mean']:.2f}ms")
        print(f"ì˜¤ë¥˜ìœ¨: {stats['error_rate']:.2f}%")
        print(f"90í¼ì„¼íƒ€ì¼ ì‘ë‹µ ì‹œê°„: {stats['response_time']['90th_percentile']:.2f}ms")
        
        # ê³„ì† ì§„í–‰ ì—¬ë¶€ í™•ì¸
        # ê³„ì† ì§„í–‰ ì—¬ë¶€ í™•ì¸
        adjusted_stats = {
            "error_rate": stats["error_rate"],
            "avg_response_time": stats["response_time"]["mean"]
        }
        should_continue, reason = controller.should_continue(adjusted_stats)
        
        if not should_continue:
            print(f"\nğŸ›‘ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {reason}")
            controller.failure_detected = True
            controller.failure_reason = reason
            break
            
        # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ì¡°ì •
        threshold_exceeded = (reason == "threshold_exceeded")
        result = controller.increment_test_parameters(threshold_exceeded)
        
        if result == "threads_increased":
            print(f"\nğŸ”„ ì“°ë ˆë“œ ìˆ˜ ì¦ê°€: {controller.current_threads}")
            print(f"   ì§€ì†ì‹œê°„ ì´ˆê¸°í™”: {controller.current_duration}ì´ˆ")
        else:
            print(f"\nâ±ï¸ ì§€ì†ì‹œê°„ ì¦ê°€: {controller.current_duration}ì´ˆ")
        
        # ë‹¨ê³„ ê°„ ì¼ì‹œ ì¤‘ì§€
        time.sleep(5)

if __name__ == "__main__":
    print("ğŸš€ JMeter ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    config = load_config()  # JSON íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ
    run_stress_test(config)